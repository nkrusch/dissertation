%! suppress = UnresolvedReference
\section{Observations About Specific Techniques}
\label{sec:res-specific}

The specific techniques are the flow calculus of mwp-bounds and the QI
Framework.

\subsection{Advancements of the Flow Calculus}
\label{subsec:res-flow-calc}

The original flow calculus of mwp-bounds~\cite{jones2009}\index{mwp-calculus}
provides a purely syntactic, sound\index{soundness} and
compositional\index{compositionality} theoretical technique for analyzing value
growth of variables in \ndx{imperative programs}. Although promising, the
technique poses various application challenges. In particular, issues arise from
the nondeterministic\index{nondeterminism} \ndx{inference rule}s and handling of
potential \ndx{derivation failure}. The developments presented in this
dissertation resolve or ease those tensions in several directions. At
conclusion, we have arrived to an \emph{enhanced} variant---\ie the
\(\mwpsc^\infty\)\symbo{mwpi} calculus---with richer capabilities and utility than the
original system.

\paragraph*{From principles to practical efficiency}
Two critical changes---in~\autoref{sec:fscd-det}---were keeping track of
derivation choices through \emph{\ndx{polynomial structures}}, and introducing
the \ndx{flow coefficient} \(\infty\)\symbo{infty} that tracks \ndx{derivation
failure}. These adjustments enables exploring all derivation paths
{concurrently}, without needing to back-track during the analysis procedure. In
the new formulation every input program is assigned precisely one \emph{complex}
\ndx{mwp-matrix}, instead of up to exponentially many \emph{simple} mwp-matrices.

Although the technical adjustments make program analysis automatable, they also
introduce a new problem. Critically, the \emph{mwp-bound}s---that represent
variable value growth and are the output of the analysis---become
\enquote{hidden} in the complex matrix. It is necessary to invent ways to
\emph{interpret} the new complex data structure to mitigate the gap.

How to completely solve this new problem is described in a second
\enquote{phase} of technical developments, in~\autoref{sec:postcond}. The
manuscript provides an {evaluation procedure} that enables recovering
\ndx{mwp-bound}s from a complex \ndx{mwp-matrix}. With this enhancement, the
\(\mwpsc^\infty\)\symbo{mwpi} calculus computes \ndx{mwp-bound}s that are
comparable to the original calculus, but in a \emph{practically efficient} way.

\paragraph*{The surpassing capabilities}
Because the complex \ndx{mwp-matrix} contains information about \emph{all}
derivations\footnote{Essentially, the complex mwp-matrix encodes a derivation
history.}, the \(\mwpsc^\infty\)\symbo{mwpi} calculus enables certain analysis
features that are not possible in the original calculus.

For example, it is possible to determine the optimal \ndx{mwp-bound}s of each
program variable. In case of \ndx{derivation failure}, it is possible to precisely
identify the variable(s) involved, and the program point that induces the
failure. It may also be possible to bound the value growth of certain variables
in presence of whole-program derivation failure. This increases
\ndx{expressiveness}, because it enables bounding more variables than what was
previously possible (refer to~\autoref{sec:performance} for comparison). A
critical step to obtain these capabilities is projecting the complex
mwp-matrices\index{mwp-matrix} on individual variables (described
in~\autoref{subsec:query}).

\paragraph*{Extracting new information from programs}
A side effect of the derivation-history tracking is that it enables extracting
\emph{new information} about the analyzed program. Although a complex mwp-matrix
encapsulates the information, a clever strategy is required to extract it. A bit
surprisingly, the evaluation procedure---developed in~\autoref{subsec:eval} to
determine \ndx{mwp-bound}s---serves as a kind of \enquote{oracle} method. We can
issue \enquote{queries} against it to answer a variety of questions about the
analyzed program. For example, the following questions can be answered.

\begin{enumerate}
\item Does a (particular) variable always fail the derivation?
\item What variables are the sources of \ndx{derivation failure}?
\item By what sequence of derivation choices is a program derivable (or not derivable)?
\item What is the optimal mwp-bound assigned to a (particular) variable?
\item What is the maximal flow coefficient between a pair of variables?
\item Is a pair of variables always independent \wrt data flow?
\item Does a derivation exist where multiple variables obtain optimal mwp-bounds?
\end{enumerate}

All of these questions can be answered in the \(\mwpsc^\infty\)\symbo{mwpi}
calculus. In the original flow calculus, answering any of them is either not
possible or would require exhaustive (infeasible) enumeration.

\paragraph*{Enhanced insights about the calculus}
The developments reveal deeper insights of the flow calculus\index{mwp-calclus}.
For example, we have now discovered that the \(0\)\symbo{zero}-coefficients are
interesting, because they track \ndx{data flow independence} between
variables.\footnote{Refer to~\autoref{dependence-analysis}
and~\autoref{subsec:ni} on the relevance of this property.}

A second insight concerns the \(w\)\symbo{w}-coefficient. The original calculus
describes it as iteration-independent\index{mwp-bound!iteration-independent},
but the significance of that term remained mysterious until recently. We now
know that, inside loops, the \(w\)\symbo{w}-coefficients corresponds to a
quasi-invariance property.\index{quasi-invariant} During iteration, it will (at
some point) become unaffected by increase in loop iteration count
(\cf~\autoref{subsec:disclaimer}).

Finally, we also know that focusing the analysis to loops---instead of functions
as in the original design---is beneficial to obtain practical analysis. In some
cases, a loop-scoped analysis enables extracting results where function-based
analysis fails (\cf~\autoref{sec:performance}). This reinforces the view that it
is justifiable to focus complexity analyses on loop
constructs~\cite{benamram2020}.

\paragraph*{Resolution of related research questions}
In response to the research questions posed in~\autoref{ssec:mwp-rqs}, we can
now confirm a positive answer to (RQ1). It is possible to obtain automatic
program analysis from the flow calculus\index{mwp-calculus} of mwp-bounds.
However, the result is not immediate, and substantial adjustment was required to
the underlying theory. The enhanced technique is applicable to automatic
resource analysis of \ndx{imperative programs}, as demonstrated by \ndx{pymwp}
in~\autoref{sec:atva}. We can also confirm the analysis results a complementary
to alternative state-of-the-art techniques~\cite[p. 5]{aubert2023b}.

Relating to (RQ3), a possible application of the flow calculus is to inference
specification\index{specifications} conditions (\cf~\autoref{sec:postcond}) in
\ndx{numerical loops}. Such use case is unconventional for complexity-based
analyses, but it should be explored more because the findings of
\autoref{sec:postcond} are encouraging. Currently, the
\(\mwpsc^\infty\)\symbo{mwpi} calculus can derive results for many kinds of
loops, including several natural algorithms (\cf~\autoref{sec:performance}).

Answering (RQ2)---a formal proof of \ndx{soundness}---still remains an
outstanding task (\cf~\autoref{sec:mwp-calc-formal}). However, the advancements
in other areas are important to motivate the formalization effort. There is now
strong evidence to justify the relevance and need for formal verification.

\subsection{Extending the QI Framework}
\label{subsec:res-qi}

In a second research direction, we applied the QI-framework to two new program
analysis applications: distributing loops for parallelization\index{loop
transformation!distribution} and tracking the security property of
\ndx{non-interference}.

\subsubsection{Loop Distributing Program Optimization}
\label{subsubsec:qi-opt}

In \enquote{Distributing and Parallelizing Non-canonical Loops}
(\autoref{sec:vmcai}), we used the QI framework to design a program optimization
that improves runtime execution profile of programs. The optimization seeks to
increase \ndx{parallelization potential} in initially sequential imperative
loops. The analysis evaluates \emph{dependence}\index{dependence analysis}
between commands. It aims to discover data flow \emph{independent}
sub-computations.\index{data flow independence} If such independence exists, we
replace the original loop with multiple new loops. The new loops can be
distributed\index{loop transformation!distribution} between the available
processors for parallel execution. The analysis is practically useful as a
compiler optimization technique.

\paragraph*{The benefits of ICC}
One uncommon aspect of the analysis is that it enables reasoning about loops whose
\ndx{iteration space} is unknown. Such loops are beyond the capabilities of
traditional parallelizing transformations. However, many ICC systems---like the
QI framework and flow calculus of mwp-bounds---over-approximate \ndx{iteration
space}. By assuming iteration is \emph{unbounded}, a loop that does
terminate\index{termination} is still within the over-approximation.
This way, the guarantees apply to all loop iteration spaces.

The cost of flexibility is reduced \ndx{precision}. However, sometimes the trade
is beneficial. For the parallelizing optimization, the property of interest is
data flow independence in and between commands. The focus is binary -- either a
data flow exists or does not exist. Beyond existence, \emph{recurrences} of data
flows are irrelevant. This enables relaxing the need to know precise loop
iteration space.\footnote{Though, determining whether loop optimization is
\emph{beneficial} requires knowledge of the loop iterations.}

\paragraph*{Moving beyond complexity}
Although the goal of the analysis is to improve running time, the optimization
is no longer defined in terms of complexity classes\footnote{This is different
from the prior use of the QI framework~\cite{moyen20172} that aimed to alter the
program's overall complexity by modifying \ndx{nested loop}s.} The application
thus moves the technique beyond complexity theory. The original complexity
result is lost, but in return, we gain a practical program optimization
technique.

\paragraph*{Resolution of related research questions}
The loop distribution technique addresses (RQ4) from~\autoref{ssec:mwp-rqs}.
Aligned with the main hypothesis, it demonstrates a new application of ICC
systems. The work is complete, but follow-up directions are discussed
in~\autoref{sec:res-future}.

\subsubsection{Detecting Non-Interference}
\label{subsubsec:qi-ni}

In subsequent work (\autoref{sec:anytime}), we explored an application of the QI
framework to track \ndx{non-interference}. Non-interference is a security
property to establish that secret information does not \enquote{\ndx{leak}} to
lower \ndx{security class}es (\cf~\autoref{pl-sec}). Although this property
\emph{seems} different from parallelizing loops, the two applications share a
common origin. Both analyses consider \ndx{data flow independence}. This makes
the QI framework suitable for both analyses.

\paragraph*{Derived insights}
Our starting point was the earlier loop parallelization formulation. One
surprising discovery was that we were able to relax that mathematical
constraints that were needed previously (refer to~\autoref{ni-composition}). For
example, the order of command composition was no longer important. For a loop
transformation, the order is important to preserve functional behavior. However,
the read-only non-interference analysis is composition-insensitive, because it
aims is to detect an information flow policy\index{information flow!policy}
\ndx{violation} \emph{anywhere} in the program. Once such leak is detected, it
is not possible to erase it.

The \ndx{non-interference} analysis allows us to evaluate generalizability of
ICC systems. Our long-held assumption is that, because the QI framework targets
a core imperative language, it can be applied to any language representation
that shares that same core. The non-interference analysis allows us to challenge
this assumption. Our plan is map our non-interference logic to two different
language representations, at different stages of compilation, to study
preservation of the \ndx{non-interference} property.

\paragraph*{Resolution of related research questions}
Our investigation into (RQ5) from~\autoref{ssec:mwp-rqs} is still ongoing.
Completing the work requires addressing certain theoretical questions (\eg
refining the definition on anytime
non-interference\index{non-interference!anytime} (\autoref{subsec:ni-soundness})
and precisely identifying its ideal application. Based on the prior success with
the QI framework, we are confident that the outstanding steps are achievable.

\subsubsection{New Insights About the QI Framework}
\label{subsubsec:qi-res}

Our investigations of the QI framework provide a case example showing that ICC
systems can be flexible enough to track various non-functional program
properties\index{non-functional property}. Although the research problems are
phrased in terms of `properties', the actual insight comes from better
understanding the underlying theory. For example, to track a complexity
property, we must compute fixed points and preserve the program command order in
transformations. In the loop distribution work and the \ndx{non-interference}
analysis, it was possible to gradually relax these requirements.

Although the QI framework appears similar to the flow calculus, there are
notable differences. Critically, the QI framework enables tracking information
in control statements. The programming language also includes arrays. There
capabilities are critical to support the adjusted analyses studied in this
dissertation.

Generalizing further, our investigations have revealed that the  QI framework is
suitable for tracking program properties that can be modeled as \emph{instances
of \ndx{data dependence}} (or independence). This is reminiscent of the
Dependency Core Calculus~\cite{abadi1999b}. Based on this observation, the QI
framework likely encapsulates even richer utility than uncovered thus far.

\section{Broader Research Findings}
\label{sec:broader-findings}

Moving beyond specific techniques, our investigation revealed observations about
connecting ICC and \ndx{automatic resource analysis}; and the challenges that
emerge in general from applying implicit computational complexity.

\subsection{Applications in Automatic Resource Analysis}
\label{subsec:res-resource-analysis}
\index{automatic resource analysis}

\paragraph*{Dissecting analysis utilities}
In the most rudimentary form, ICC-based program analyses evaluate a binary
membership problem. The task is to decide whether an input program belongs to a
targeted complexity class\index{complexity class}. Like in \ndx{safe recursion}
(\autoref{safe-rec}), no explicit bound needs to be provided~\cite[p.
13]{moyen2017}.

Although such binary result is coarse in the amount of information it provides,
it does have applications. For example, Karp~\cite{zhang2022} is a
\ndx{domain-specific language} (DSL) for programming and testing \ndx{Karp
reduction}s. A Karp reduction must be polynomial time, but no explicit bounds
are required. For applications in \ndx{cryptography}, the situation is reversed.
A critical step is showing that secret communications have desired properties
despite of the best efforts of an untrusted party~\cite{rivest1990}. A guarantee
of secret communication then requires showing that no feasible (polynomial-time)
algorithm exists for breaking the security mechanism. Yet, these are isolated
use cases; a coarse binary analysis has limited utility in general.

The practical relevance of an analysis increases if we can show it can support
software engineering or verification tasks. Such analyses produce quantitative
information that helps engineers understand the program behavior, and detect and
repair issues early. Ideally, an analysis should express \ndx{resource bounds}
and flag resource-intensive procedures. Certain ICC systems---like the flow
calculus of mwp-bounds---can target such the practically-oriented applications
up to an extent. However, if some computation exceeds polynomial bounds, the
analysis should still yield an informative result. This last requirement is
often beyond capabilities of ICC systems~\cite{baillot2012}.

On the analysis input side, the utility improves with expressive
power\index{expressiveness}. Essentially, we would want the analyzer to handle
the rich constructs that occur in real-world programs. ICC systems are typically
defined on core languages (\enquote{toy} languages). However, it is possible to
work around this limitation by focusing on interesting sub-classes of programs,
\eg integer programs or \ndx{numerical loops}. Through
compositional\index{compositionality} analysis of program \emph{fragments}, ICC
systems extend to analysis of general programming languages. This approach is
demonstrated in pymwp (\autoref{sec:atva}). Having a restricted syntax is thus
not an insurmountable hurdle to applications, in our experience. To make an
analysis comparable with many existing resource analyzers\index{resource
analysis}~\cite{flores17}, it suffices to target the \ndx{C} Integer Programs
\ndx{grammar}~\cite{cinteger} used in the \ndx{Termination
Competition}~\cite{giesl2019}.

\paragraph*{Motivating analyzer developments}
A second observation is about the resources analyzers themselves. Scientific
software is different from \enquote{mainstream} software in several ways. For
example, scientific software aims to materialize theoretical ideas that
correspond to a publication. After presentation, the software is
archived~\cite{acm_badging}, while authors move on to the next prototype. This
description is not meant as criticism; rather, it is to make explicit how
scientific software creation is incentivized.

One community that operates differently is formal methodsâ€”--see as
evidence~\autoref{tab:fm-tools}). Many formal verification tools are developed
with continuity in mind, through annual competitions~\cite{casc,beyer2022},
\etc. Compared to tools in \ndx{resource analysis}, those goals are not that
distant, as resource analyzers also have similar competitions. Though,
unfortunately, the \enquote{software situation} is still sub-optimal for
resource analyzers.

Since resource analyzers tend to be \enquote{standalone} tools, their
development is not as strongly incentivized as \eg SMT solvers. Certain
analyzers, although they regularly appear in literature, are
inaccessible~\cite{sinn2017}, difficult to locate~\cite{carbonneaux2015}, or
deprecated~\cite{gulwani2009,srikanth2017}. The resource analyzers also differ
greatly in their input/output formats. This is problematic because it is
difficult to faithfully compare new techniques to existing ones. One big idea,
that would foster improvement, is finding ways to \emph{integrate} resource
analyzers to engineering workflows. Specifically (as hinted at
in~\autoref{sec:postcond}), resource analyzers could assist in deductive
verification.

\subsection{Thinking Outside the (Complexity) Box}
\label{subsec:res-other-props}

ICC can perform at any level, and may even be able to prove that some properties are maintained.
Do we want to also give examples of properties that are *not* preserved by the chain;
typically, constant-time properties may be broken by some optimizations.
Through compositional analysis of subprograms, ICC systems extend to analysis of general programs.
Besides complexity, ICC systems can be adjusted to track other semantic properties.
%Research questions 4 and 5 are posed here with an air of obviousness.
%However, arriving to those questions was not obvious.
%A substantial challenge in the research plan involves identifying suitable research domains---like parallel programming or language-based security---
%where ICC techniques could offer meaningful benefit.

\subsection{The \emph{Implicit Costs} of Applied ICC}
\label{subsec:res-meta}

The dissertation manuscripts follow a pattern of pairing ICC with a secondary application domain.
Although such intersectional strategy comes with rich potential, there are also substantial \enquote{costs}.
One cost relates to a learning overhead.

The overhead behind the dissertation manuscripts is reflected in \autoref{sec:pre}.
Making a meaningful contribution requires first sufficient familiarity with secondary domain to identify research gaps.
It also requires learning the terminology and implicit expectations of the secondary domain.
For example, in presenting developments of the flow calculus, we learned through trial-and-error that experiments were an absolute prerequisite to support our findings.

The second cost is about understanding different measures of significance.
While we hold the theoretical origins in high esteem, it becomes minutiae when targeting applications in other domains.
Using an ICC technique to solve a problem in ways that is comparable to existing techniques is not enough for a scientific contribution.
The significance of the application must be standalone and independent of the theoretical origin.
In other words, a technique becomes interesting only after we can show it can solve an interesting problem.

The described research strategy has repeated a few times throughout the dissertation, multiplying these costs.
In reflection, following the strategy requires persistence, and in general not ideal for dissertation research.

\section{Open Problems for Future Work}
\label{sec:res-future}

\subsection{Questions About the Flow Calculus}
\label{ssec:mwp-open}

The following questions remain unanswered about the flow calculus of
mwp-bounds\index{mwp-calculus}.

\begin{itemize}
\item \emph{Have the developments changed the complexity of the derivability
        problem?}\footnote{The problem is \ccx{np}-complete by~\cite[p.
        37]{jones2009}).}
      The manuscripts in \autoref{sec:fscd},~\autoref{sec:atva},
      and~\autoref{sec:postcond} alter the inference procedure, therefore the
      answer is not obvious.

\item \emph{How to obtain concrete bounds from \ndx{mwp-bound}s?}
      The honest polynomials of \ndx{mwp-bound}s are composed of variables,
      constants, and operators, but how to extract the exact combination is
      unclear.

\item \emph{Could the flow-calculus also capture lower bounds?}
      A lower bounds analysis would complement the existing capabilities by
      detecting variables that have certainly exponential value growth, which
      is useful \eg for bug finding.

\item \emph{Issues around translating real-world programming languages to the
      imperative language of the flow calculus.}
      Although the flow calculus inference rules are phrased as \pr|while| and
      \pr|loop| commands, they should be interpreted as \emph{unbounded} and
      \emph{bounded} loops. An ideal translation should compile the input
      program to the language of the flow calculus, while accounting for \ndx{loop
      boundedness}. A solution might be already available in the
      compilers literature.

\item \emph{How to account for various rich program constructs,
      like arrays?}
     Looping over arrays is a frequently occurring programming pattern.
     Although this is a natural extension to the flow calculus,
     there is no existing approach to this problem.

\end{itemize}

As evidenced in these open programs, the flow calculus of mwp-bounds is
interesting because it provides an extensible and rich logical framework for
reasoning. Its full potential still remains to be explored.

\subsection{Other Emerging Research Questions}
\label{ssec:other-open}

\paragraph*{Generalizing the loop distribution}
The solution of~\autoref{sec:vmcai} assumes a setting where loop distribution
should be applied. Relaxing this assumption raises at least two new relevant
questions. Determining that a loop distribution should be applied requires first
determining the profitability of the operation. Only if the operation is
beneficial (by some metric), should the loops be distributed. Moreover, compiler
loop transformations do not occur in isolation. Another concerns thus involves
relation to the auxiliary transformations. Exploring these questions is a
natural continuation of the prior work.

\paragraph*{Complexity-by-Construction}
The \ndx{X-by-Construction} paradigm~\cite{terbeek2018} is concerned with
ensuring non-functional properties\index{non-functional property} by
construction. The XbC aim is to automatically generate error-free software from
\ndx{specifications}~\cite{terbeek2020}. ICC systems define the programming
language-based mechanics to construct polytime programs, therefore it is
conceivable that ICC systems could extend the XbC approach in cases where X
stands for complexity. It would require pairing ICC techniques with \ndx{program
synthesis}. Conceptually, this is the \enquote{bottom-up} dual of the
\enquote{top-down} approach implemented in \ndx{pymwp}.

\paragraph*{Formally verifying complexity}
ICC systems are based on programming languages, making them naturally suited
to formalization. However, the number of works that support this claim is
still modest---refer to \eg~\cite{feree2018,heraud2011,atkey2024}. The
investigation is necessary to have formal guarantees of complexity. There
are two separate questions (i) the formalization of logical systems that
provide guarantees, and (ii) formalizing analyzers that provide complexity
guarantees. The existing works address the first question. The closes
solution to the second is giving guarantees to the results computed by an
analyzer~\cite{carbonneaux2017}.

\paragraph*{Extending beyond traditional programming paradigms}
ICC has been widely studied in the context of \enquote{traditional} programming
paradigms, including imperative and functional programs.\footnote{ For example,
the works building on Resource Aware ML\index{RaML}~\cite{hoffmann2012} have
been extended to many paradigms.} However, there are many more important
programming paradigms, like \ndx{probabilistic programs} and \ndx{quantum
computing}, that warrant investigation. These paradigms change the notion of
programming languages and thus require different analytical approaches.
Representative works exploring this direction
include~\cite{dallago2010,avanzini2024,colledan2024,avanzini2020}.

\section{Completion of Dissertation Goals}
\label{sec:res-summary}

As part of the dissertation aims, Section~\autoref{subsec:specific-aims} outlined
four goals. We can now assess the completion status of the goals.

\begin{itemize}

\item[\iconDONE{ }]\textbf{Goal 1.}
\emph{Extend applied capabilities in automatic program analysis and
verification.} The works extending the flow calculus of mwp-bounds---in Sections
\autoref{sec:fscd},~\autoref{sec:atva}, and~\autoref{sec:postcond})---and the
loop distribution technique of~\autoref{sec:vmcai}, immediately support the
goal. Each manuscript takes lifts implicit computational complexity beyond its
theoretical origins, and demonstrates its uses in an applied context. The
complementary nature of these techniques is discussed
in~\autoref{sssec:atva-originalities} (see in
particular~\autoref{tab:atva-compare}), \autoref{sec:vmcai-limitations}
and~\autoref{sec:related-works}. Therefore, the goal was met satisfactorily.

\item[\iconPROG]\textbf{Goal 2.}
\emph{Take ICC techniques closer to integration with real-world software
development workflows.} The dissertation work is consistently supported by
software artifacts (\cf~\autoref{app:sec:artifacts}). While the artifacts
demonstrate the practical relevance of the investigations, and have driven
advancements in the underlying theories, the results so far are still a step in
\enquote{isolation}. Integrating techniques based on implicit computational
complexity into other software development tools (compilers, verifiers, \etc) is
important to promote relevance and continue advancement of ICC\@. Such ambitions
encourage thinking about ICC from new (more practical) perspectives. However,
concrete integration remains as an outstanding goal. The work in using the flow
calculus\index{mwp-calculus} to infer specification\index{specifications}
conditions~\cite{rusch2025} is one step in that direction.

\item[\iconPROG]\textbf{Goal 3.}
\emph{Initiate conversations about the relevance of ICC applications.} The third
goal is \enquote{internal} to those who are already familiar with ICC. The
dissertation introduction (\ie \enquote{Addressed Problem}
in~\autoref{subsec:problem}), and the introduction to ICC (\enquote{A Snapshot
of Theoretical Results} in~\autoref{icc-theories}), both highlighted the strong
preference for pure theoretical development. Taking inspiration
from~\cite{moyen2017}, the motivation is to push theoretical developments beyond
this pure framing. There was some progress in initiating conversations about
applications---via~\cite{aubert20222} and a presentation at the Seminar on
Semantic and Formal Approaches to Complexity~\cite{scot23})---but the overall
effort toward the goal was limited. Although this outcome is less than ideal,
the motivations presented in the published works, and this dissertation, will
remain discoverable to others in perpetuity.

\item[\iconDONE]\textbf{Goal 4.}
\emph{Expose ideas from implicit computational complexity to broader research
communities.} The dissemination of the dissertation work involved many written
works and giving presentations to diverse audiences. The \enquote{themes} of the
presentations venues included type theory (the TYPES'22
conference~\cite{aubert202217}), interactive theorem proving
(CoqPL'23~\cite{aubert20231}), language-based security
(PLAS'24~\cite{plas2024}), and programming languages (at the doctoral
symposiums of SPLASH'22~\cite{splash22} and ECOOP'25~\cite{rusch20257}), to name
a few. The number of attendees at these events was in the magnitude of hundreds.
Based on observation, implicit computational complexity was largely an
unfamiliar topic to the audiences. Conversely, through the different modes of
dissemination, a substantial effort was made to increase exposure and awareness
of implicit computational complexity. Therefore, the final goal was
satisfactorily met.

\end{itemize}

Although the goals were completed with a mixed degree of success, there was
progress in every direction. The goals are intentionally open-ended and need not
be fully achieved within the timespan of doctoral studies. It is always possible
to push further, even in the cases that were completed satisfactorily.

\section{Research Conclusions}
\label{ssec:findings}

We can now summarize the key takeaways of this dissertation.

\begin{infobox}{Primary findings}
\begin{enumerate}[wide, labelwidth=!, labelindent=0pt]

\item Implicit computational complexit (ICC) provides complementary
techniques of \ndx{automatic resource analysis}.

\item ICC systems are flexible and can be modified to track other
non-functional properties, beyond complexity. \index{non-functional
property}

\item Although this dissertation has provided early evidence, continued
exploration is needed to unlock further application potential.

\end{enumerate}
\end{infobox}

The first finding recognizes the fact that Implicit computational complexity
offers complementary techniques to perform \ndx{automatic resource analysis}
(\cf~\autoref{sec:fscd} and~\autoref{sec:atva}). However, such applications may
require substantial adjustment to the base theory. While designing ICC systems,
it is therefore important to consider practical challenges ahead of time.

The second finding emphasizes \emph{extensibility}. Starting with an ICC system,
it is possible to adjust it to track alternate properties. The value of such
exploration is two-fold. Due to the unusual starting point, the obtained
analysis may be complementary to existing techniques (as in
\autoref{sec:vmcai}). Moreover, the adjustment provides new insights of the
technique itself.

The third finding is reflective. The dissertation investigation has explored
many applications of ICC\@. However, investigation has been far from exhaustive
and many more applications remain to be discovered. In particular, the
guarantees ICC can offer by construction should be investigated further. Yet,
such exploration necessitates a viewpoint shift and seeing ICC as more than
\enquote{just} a complementary approach to complexity theory. When viewed
broadly, ICC systems become rich reasoning frameworks that allow analyzing and
verifying wide varieties of program properties.

