\section{Observations About Specific Techniques}
\label{sec:res-specific}

\subsection{Advancements of the Flow Calculus}
\label{subsec:res-flow-calc}

The original flow calculus of mwp-bounds provides a purely syntactic, sound and compositional theoretical technique for analyzing value growth of variables in imperative programs.
Although promising, the technique poses various challenges for applications.
In particular, issues arise from nondeterministic inference rules and handling of potential derivation failure.
The developments presented in this dissertation resolve or ease those tensions in several directions.
At conclusion, we obtain an enhanced flow calculus with richer capabilities and utility than the original system.

The essential technical changes involve introducing an \(\infty\)-coefficient (\cf~\aref{sec:fscd}) that tracks derivation failure.
It enables exploring all derivation paths {concurrently}, \ie without the need to back-track during program analysis procedure.
Then,---instead of 0 to exponentially many simple matrices---every analyzed program is characterized by precisely one \emph{complex} mwp-matrix.
Although the introduction of the \(\infty\)-coefficient (and the related technical adjustments) make program analysis automatable,
they also introduce a new problem.
The mwp-bounds, that represent variable value growth, are \enquote{lost} in such a complex matrix until
we discover ways to \emph{interpret} the new complex data structures.

How to completely solve this new problem is described in a second phase of technical developments (\cf~\autoref{sec:postcond}).
The manuscript provides an \emph{evaluation procedure} that enables recovering the mwp-bounds from a complex mwp-matrix.
However, since the matrix contains information about all derivations, it is now possible to also obtain the \emph{optimal} mwp-bounds of each program variable.
A critical step is to \emph{project} mwp-matrices on individual variables.
The projection also enables bounding certain variables in presence of whole-program derivation failure.
This leads to increased \ndx{expressiveness} in abilities to bound more variables than in the original system (\cf~\autoref{sec:performance}).

One fascinating side-effect of the enhanced calculus is that it enables extracting \emph{new information} about the analyzed program.
A complex mwp-matrix encapsulates this information, but requires a clever strategy to extract it.
To this end, we view the evaluation procedure as a kind of \enquote{oracle} method, against which we can issue queries (\cf~\autoref{sec:pc-analysis}).
For example, it is possible to pose questions such as the following.
\begin{enumerate}
\item Does a variable always fail the derivation?
\item What variables are sources of failure?
\item By what sequence of derivation choices is a program (not) derivable?
\item What is the optimal mwp-bound assigned to a variable?
\item What is the maximal coefficient of dependency between a pair of variables?
\item Is a pair of variables independent \wrt data flow?
\item Does a derivation exist where many variables obtain optimal mwp-bounds?
\end{enumerate}
All of these questions can be answered in the enhanced calculus.
However, in the original flow calculus, answering \emph{any} of them is either outside the capabilities or would require exhaustive enumeration, which is infeasible.

The developments also reveal deeper insights of the technique.
For example, we now know that the \(0\)-coefficients are interesting, because they track data-flow independence between variables
(refer to~\autoref{dependence-analysis} and~\autoref{subsec:ni} on the relevance of this property).
Although the original formulation of the flow calculus notes that the \(w\)-coefficient is iteration-independent, the significance of that term remained mysterious.
We now know that when a variable is characterized by at most \(w\)-coefficients, the variable is subject to a loop
quasi-invariance property, and will (at some point) become unaffected by increase in loop iteration count (\cf~\autoref{sec:pc-analysis}).
Finally, we also know that focusing the analysis to loops (instead of functions as in the original design) is beneficial for practical analysis.
The modification enables extracting results in some cases where function-based analysis fails (\cf~\autoref{sec:performance}).
This reinforces the view that it is justified to limit complexity analyses on loop constructs~\cite{benamram2020}.

To summarize, the enhanced \textsc{mwp}\(^\infty\)-calculus makes the flow calculus automatable for static program analysis,
but is also richer in the information it provides.
The enhanced technique is applicable in automatic analysis of complexity properties of imperative programs, as shown in~\autoref{sec:atva}.
Another use case is providing assistance in formal specification conditions inference (\cf~\autoref{sec:postcond}).
The latter is an unconventional application of complexity-based analyses, but one that \emph{should be} explored more --
the pioneering results of~\autoref{sec:postcond} are encouraging.
At current stage, the flow calculus can derive results efficiently, including for several natural algorithms (\cf~\autoref{sec:performance}).
These results are significant to motivate the relevance of formally verifying the core calculus (\cf~\autoref{sec:mwp-calc-formal}).

% TODO: did it cover these points?
% We can now confirm that it is possible to obtain automatic program analysis from the flow calculus of mwp-bounds, but \emph{only after substantial adjustments}.
% An efficient program analysis requires inventing ways to manage costly sub-computations and solving additional problems that arise in the process.
% At conclusion, the enhanced technique is not just automatable, but can analyze more programs than the original theory.
% We can now also confirm the analysis results a complementary to alternative state-of-the-art techniques~\cite[p. 5]{aubert2023b}, enabling us to analyze different
% questions about resource consumption.
% The implemented flow calculus can also be used in specification inference.
% There are potentially many other applications remaining to be discovered.
% However, our experience of developing the flow calculus strongly reinforced the symbiotic view of theory and application.
% In the process, we faced scientific and engineering challenges, but also identified compelling arguments to justify the investigation.
% At conclusion, we have strengthened the theory and clarified its relevance.

\subsection{Extended Properties via Quasi Invariants}
\label{subsec:res-qi}

We have applied the quasi invariants framework in two use cases: to program optimization and tracking non-interference.

\subsubsection{Use in Program Optimization}
\label{subsubsec:qi-opt}

In \enquote{Distributing and Parallelizing Non-canonical Loops},
we used the QI framework to design a program optimization to improve programs' runtime execution profile.
The optimization seeks to increase parallelization \emph{potential} in initially sequential imperative loops.
The analysis evaluates \emph{dependence} between commands to discover \emph{independent} sub-computations.
If such independence exists, we replace the original loop with multiple new loops.
The new loops can be distributed between available processors for parallel execution.
The analysis is intended for use as a compiler optimization technique.

The exception attribute of the analysis is that it enables reasoning about
loops whose iteration count is unknown.
Such loops are beyond the capabilities of traditional parallelizing transformations.
However, many ICC systems---including the flow calculus of mwp-bounds---over-approximate loop iterations by assuming them to be infinite\footnote{If a loop terminates earlier, the behavior is still within the over-approximation.}.
This way, the provided guarantees apply across all conceivable iteration behaviors.
The cost of the flexibility is reduced precision;
however, sometimes the trade is beneficial.
For the parallelizing optimization, the property of interest is \emph{independence} of data flow in and between commands.
Therefore, the focus is binary, \ie whether a data flow exists.
Beyond existence, recurrences of data flow are irrelevant.
This enables relaxing the need to know loop iteration count.

Although the goal of the analysis is to improve running time,
the optimization is no longer defined in terms of complexity classes\footnote{
This is different from the prior use of the QI framework~\cite{moyen20172}
that aimed to alter the program's overall complexity by modifying nested loops.}
Thus, in this work, we shift the analysis technique outside complexity theory.
The original complexity result is lost, but we gain a practical program optimization in return.

\subsubsection{Use in Non-Interference}
\label{subsubsec:qi-ni}

In subsequent work (\autoref{sec:anytime}) we have explored  using the QI framework to track non-interference.
Non-interference is a security property for establishing that secret information does not \enquote{leak} to lower security classes (\cf~\autoref{pl-sec}).
Although this property \emph{seems} different from parallelizing loops, the two applications have a clear commonality.
Both are about data flow independence.
Therefore, the QI framework is suitable for both analyses.

We started developing a non-interference logic, taking as a starting point the loop parallelization formulation.
A surprising discovery that soon followed, was that we could relax many of the mathematical constraints that were needed previously.
For example, the order of command composition is important if we want to transform loops and preserve the loop's functional behavior.
For the non-interference analysis, which is read-only, the relevant question is whether an information flow leak occurs.
Once such leak is detected, it is not possible to erase it.
Therefore, the analysis becomes \enquote{composition-insensitive}.
Overall, it was enlightening how changing the evaluated property altered the analysis mechanics.

The non-interference analysis also provides us an opportunity to evaluate a yet another aspect about ICC systems.
Our long-held assumption is that, because the QI framework targets a core imperative \pr|while| language;
it can be applied to any language representation that shares that same core\footnote{The same argument applies to the flow calculus of mwp-bounds.}.
Through non-interference analysis, we can actually challenge this assumption.
We plan to map our non-interference logic to two different language representations, at different stages of compilation.
The intuition is to study whether (or how) the non-interference property is preserved during compilation.
Assuming the idea is achievable, it gives us a different perspective to reason about non-interference.
The typical approach is using a type system, with a singular type check.
We are optimistic that the idea could work, and if so,
we can reinforce the bi-directionality (\cf~\autoref{icc-sec}) between ICC and language-based security.

% TODO: general comments, based on those two projects
% The QI framework provides a case example showing that ICC systems can be flexible enough to track other program properties.
% While adjusting the system, we made a surprising discovery in that changing the property of interest simplified the mathematical analysis.
% For example, to track a complexity property, we must compute fixed points and preserve the program command order.
% In the adjusted analyses, it was possible to relax these conditions.
% More generally, that the QI framework can be used to track various properties is reminiscent of the Dependency Core Calculus (DCC)~\cite{abadi1999b}.
% In DCC, different program properties are modeled as \emph{instances} of a central notion of \emph{dependency}.
% This mirrors our observations about the QI framework.
% This suggests that the QI framework encapsulates even wider utility than uncovered during this dissertation work.

\section{Broader Research Findings}
\label{sec:broader-findings}

\subsection{Applications in Automatic Resource Analysis}
\label{subsec:res-resource-analysis}

Our investigations revealed several broad observations about automatic resource analysis.
The first is about the analysis results we can obtain with implicit computational complexity, and how to judge the application potential of ICC systems.
Centrally, ICC systems aim to confirm whether a program belongs to some complexity class.
There are \emph{some} use cases for such binary classification.
For example, the domain-specific language Karp~\cite{zhang2022}---for programming and testing \ndx{Karp reduction}s---naturally requires ways to confirm a reduction is computable in polynomial time (without needing further precision).
However, it is an isolated use case.
A coarse binary analysis has limited practical utility in general.

The relevance of an analysis increases if we can show it can assist in software engineering tasks.
Such analysis should produce quantitative information that helps engineers to understand the program behavior.
The analysis should express resource bounds and pinpoint issues like resource-intensive procedures.
Certain ICC systems, like the flow calculus of mwp-bounds, have these capabilities naturally.
If some computations exceed polynomial bounds, the analysis should still yield (some) informative results.
This last requirement is often beyond capabilities of ICC systems~\cite{baillot2012}, including the original formulation of the flow calculus.
Although many ICC systems are typically defined on \enquote{little} languages, it is possible to work around this limitation by focusing on sub-classes of programs, \eg integer programs and numerical loops.
Through compositional analysis of subprograms, ICC systems extend to analysis of general programming languages, as demonstrated with pymwp.
In other words, having a restricted syntax is not an insurmountable hurdle to applications, in our experience.
To make an analysis comparable to existing resource analyzers, it suffices to target the \ndx{C} Integer Programs \ndx{grammar}~\cite{cinteger} used in the Termination Competition~\cite{giesl2019}.
From these attributes, we can derive a checklist to judge the application potential of various ICC systems.

A second observation is about the resources analyzers themselves.
Scientific software is different from \enquote{mainstream} software in several ways.
For example, scientific software aims to materialize theoretical ideas that correspond to a publication.
After presentation, the software is archived~\cite{acm_badging}, while authors move on to the next prototype.
This description is not meant as criticism;
rather, it is to make explicit how software creation is incentivized in scientific context.
One community that operates differently is formal methodsâ€”--see as evidence~\autoref{tab:fm-tools}).
Many formal verification tools are developed with continuity in mind, through annual competitions~\cite{casc,beyer2022}, \etc
Compared to resource analysis, those goals are not that distant, as resource analyzers also have similar competitions.
Though, unfortunately, the \enquote{software situation} is still sub-optimal for resource analyzers.
Since resource analyzers tend to be \enquote{standalone} tools, their development is not as strongly incentivized as \eg SMT solvers.
Certain analyzers, although they regularly appear in literature, are inaccessible~\cite{sinn2017}, difficult to locate~\cite{carbonneaux2015},
or deprecated~\cite{gulwani2009,srikanth2017}.
The resource analyzers also differ greatly in their APIs.
This is problematic because it is difficult to faithfully compare new techniques to existing ones.
One big idea, that would foster improvement, is finding ways to \emph{integrate} resource analyzers to engineering workflows.
Specifically, as hinted at in~\autoref{sec:postcond}, resource analyzers can assist in deductive verification.

\subsection{Thinking Outside the (Complexity) Box}
\label{subsec:res-other-props}

ICC can perform at any level, and may even be able to prove that some properties are maintained.
Do we want to also give examples of properties that are *not* preserved by the chain;
typically, constant-time properties may be broken by some optimizations.
Through compositional analysis of subprograms, ICC systems extend to analysis of general programs.
Besides complexity, ICC systems can be adjusted to track other semantic properties.
%Research questions 4 and 5 are posed here with an air of obviousness.
%However, arriving to those questions was not obvious.
%A substantial challenge in the research plan involves identifying suitable research domains---like parallel programming or language-based security---
%where ICC techniques could offer meaningful benefit.

\subsection{The \emph{Implicit Costs} of Applied ICC}
\label{subsec:res-meta}

The dissertation manuscripts follow a pattern of pairing ICC with a secondary application domain.
Although such intersectional strategy comes with rich potential, there are also substantial \enquote{costs}.

One cost relates to a learning overhead.
The overhead behind the dissertation manuscripts is reflected in \autoref{sec:pre}.
Making a meaningful contribution requires first sufficient familiarity with secondary domain to identify research gaps.
It also requires learning the terminology and implicit expectations of the secondary domain.
For example, in presenting developments of the flow calculus, we learned through trial-and-error that experiments were an absolute prerequisite to support our findings.

The second cost is about understanding different measures of significance.
While we hold the theoretical origins in high esteem, it becomes minutiae when targeting applications in other domains.
Using an ICC technique to solve a problem in ways that is comparable to existing techniques is not enough for a scientific contribution.
The significance of the application must be standalone and independent of the theoretical origin.
In other words, a technique becomes interesting only after we can show it can solve an interesting problem.

The described research strategy has repeated a few times throughout the dissertation, multiplying these costs.
In reflection, following the strategy requires persistence, and in general not ideal for dissertation research.

\section{Open Problems and Future Opportunities}
\label{sec:res-future}

\subsubsection{About the Flow Calculus}
\label{ssec:mwp-open}

The following questions remain unanswered about the flow calculus of
mwp-bounds\index{mwp-calculus}.

\begin{itemize}
\item \emph{Have the developments changed the complexity of the derivability
        problem?}\footnote{The problem is \ccx{np}-complete by~\cite[p.
        37]{jones2009}).}
      The manuscripts in \autoref{sec:fscd},~\autoref{sec:atva},
      and~\autoref{sec:postcond} alter the inference procedure, therefore the
      answer is not obvious.

\item \emph{How to obtain concrete bounds from \ndx{mwp-bound}s?}
      The honest polynomials of \ndx{mwp-bound}s are composed of variables,
      constants, and operators, but how to extract the exact combination is
      unclear.

\item \emph{Could the flow-calculus also capture lower bounds?}
      A lower bounds analysis would complement the existing capabilities by
      detecting variables that have certainly exponential value growth, which
      is useful \eg for bug finding.

\item \emph{Issues around translating real-world programming languages to the
      imperative language of the flow calculus.}
      Although the flow calculus inference rules are phrased as \pr|while| and
      \pr|loop| commands, they should be interpreted as \emph{unbounded} and
      \emph{bounded} loops. An ideal translation should compile the input
      program to the language of the flow calculus, while accounting for \ndx{loop
      boundedness}. A solution might be already available in the
      compilers literature.

\item \emph{How to account for various rich program constructs,
      like arrays?}
     Looping over arrays is a frequently occurring programming pattern.
     Although this is a natural extension to the flow calculus,
     there is no existing approach to this problem.

\end{itemize}

As evidenced in these open programs, the flow calculus of mwp-bounds is
interesting because it provides an extensible and rich logical framework for
reasoning. Its full potential still remains to be explored.

\subsubsection{Other Emerging Research Questions}
\label{ssec:other-open}

\paragraph*{Complexity-by-Construction}
The \ndx{X-by-Construction} paradigm~\cite{terbeek2018} is concerned with
ensuring non-functional properties\index{non-functional property} by
construction. The XbC aim is to automatically generate error-free software from
\ndx{specifications}~\cite{terbeek2020}. ICC systems define the programming
language-based mechanics to construct polytime programs, therefore it is
conceivable that ICC systems could extend the XbC approach in cases where X
stands for complexity. It would require pairing ICC techniques with \ndx{program
synthesis}. Conceptually, this is the \enquote{bottom-up} dual of the
\enquote{top-down} approach implemented in \ndx{pymwp}.

\paragraph*{Formally verifying complexity}
ICC systems are based on programming languages, making them naturally suited
to formalization. However, the number of works that support this claim is
still modest---refer to \eg~\cite{feree2018,heraud2011,atkey2024}. The
investigation is necessary to have formal guarantees of complexity. There
are two separate questions (i) the formalization of logical systems that
provide guarantees, and (ii) formalizing analyzers that provide complexity
guarantees. The existing works address the first question. The closes
solution to the second is giving guarantees to the results computed by an
analyzer~\cite{carbonneaux2017}.

\paragraph*{Extending beyond traditional programming paradigms}
ICC has been widely studied in the context of \enquote{traditional} programming
paradigms, including imperative and functional programs.\footnote{ For example,
the works building on Resource Aware ML~\cite{hoffmann2012} have been extended
to many paradigms.} However, there are many more important programming
paradigms, like \ndx{probabilistic programs} and \ndx{quantum computing}, that
warrant investigation. These paradigms change the notion of programming
languages and thus require different analytical approaches. Representative works
exploring this direction include~\cite{avanzini2024,colledan2024,avanzini2020}.

\section{Dissertation Goal Completion}
\label{sec:res-summary}

As part of the dissertation aims, Section~\autoref{subsec:specific-aims} outlined
four goals. We can now assess the completion status of the goals.

\begin{itemize}

\item[\iconDONE{ }]\textbf{Goal 1.}
    \emph{Extend applied capabilities in automatic program analysis and
      verification.}
    The works extending the flow calculus of mwp-bounds---in Sections
    \autoref{sec:fscd},~\autoref{sec:atva}, and~\autoref{sec:postcond})---and
    the loop distribution technique of~\autoref{sec:vmcai}, immediately support
    the goal. Each manuscript takes lifts implicit computational complexity
    beyond its theoretical origins, and demonstrates its uses in an applied
    context. The complementary nature of these techniques is discussed
    in~\autoref{sssec:atva-originalities} (see in
    particular~\autoref{tab:atva-compare}), \autoref{sec:vmcai-limitations}
    and~\autoref{sec:related-works}. Therefore, the goal was met satisfactorily.

\item[\iconPROG]\textbf{Goal 2.}
    \emph{Take ICC techniques closer to integration with real-world software
      development workflows.}
    The dissertation work is consistently supported by software artifacts
    (\cf~\autoref{app:sec:artifacts}). While the artifacts demonstrate the
    practical relevance of the investigations, and have driven advancements in
    the underlying theories, the results so far are still a step in
    \enquote{isolation}. Integrating techniques based on implicit computational
    complexity into other software development tools (compilers, verifiers,
    \etc) is important to promote relevance and continue advancement of ICC\@.
    Such ambitions encourage thinking about ICC from new (more practical)
    perspectives. However, concrete integration remains as an outstanding goal.
    The work in using the flow calculus\index{mwp-calculus} to infer
    specification\index{specifications} conditions~\cite{rusch2025} is one step
    in that direction.

\item[\iconPROG]\textbf{Goal 3.}
    \emph{Initiate conversations about the relevance of ICC applications.}
    The third goal is \enquote{internal} to those who are already familiar with
    ICC. The dissertation introduction (\ie \enquote{Addressed Problem}
    in~\autoref{subsec:problem}), and the introduction to ICC (\enquote{A
    Snapshot of Theoretical Results} in~\autoref{icc-theories}), both
    highlighted the strong preference for pure theoretical development. Taking
    inspiration from~\cite{moyen2017}, the motivation is to push theoretical
    developments beyond this pure framing. There was some progress in initiating
    conversations about applications---via~\cite{aubert20222} and a presentation
    at the Seminar on Semantic and Formal Approaches to
    Complexity~\cite{scot23})---but the overall effort toward the goal was
    limited. Although this outcome is less than ideal, the motivations presented
    in the published works, and this dissertation, will remain discoverable to
    others in perpetuity.

\item[\iconDONE]\textbf{Goal 4.}
    \emph{Expose ideas from implicit computational complexity to broader research
          communities.}
    The dissemination of the dissertation work involved many written works and
    giving presentations to diverse audiences. The \enquote{themes} of the
    presentations venues included type theory (the TYPES'22
    conference~\textcite{aubert202217}), interactive theorem proving
    (CoqPL'23~\textcite{aubert20231}), language-based security
    (PLAS'24~\textcite{plas2024}), and programming languages (at the doctoral
    symposiums of SPLASH'22~\cite{splash22} and ECOOP'25~\cite{rusch20257}), to
    name a few. The number of attendees at these events was in the magnitude of
    hundreds. Based on observation, implicit computational complexity was
    largely an unfamiliar topic to the audiences. Conversely, through the
    different modes of dissemination, a substantial effort was made to increase
    exposure and awareness of implicit computational complexity. Therefore, the
    final goal was satisfactorily met.

\end{itemize}

Although the goals were completed with a mixed degree of success, there was
progress in every direction. The goals are intentionally open-ended and need not
be fully achieved within the timespan of doctoral studies. It is always possible
to push further, even in the cases that were completed satisfactorily.

\section{Research Conclusions}
\label{ssec:findings}

We can now summarize the key takeaways of this dissertation.

\begin{infobox}{Main findings}
\begin{enumerate}[wide, labelwidth=!, labelindent=0pt]

\item Implicit computational complexit (ICC) provides complementary
techniques of \ndx{automatic resource analysis}.

\item ICC systems are flexible and can be modified to track other
non-functional properties, beyond complexity. \index{non-functional
property}

\item Although this dissertation has provided early evidence, continued
exploration of ICC is necessary to unlock its full application potential.

\end{enumerate}
\end{infobox}

First, Implicit computational complexity offers complementary techniques of
automatic resource analysis. However, applications of ICC may require
substantial adjustment to the theory. It is important to account for practical
challenges in the design of ICC systems. Second, ICC systems can be adjusted to
tracking \emph{other} program properties, beyond complexity theory. Such
adjusted ICC systems can provide analysis techniques in cases that are outside
the capabilities of the established techniques. Finally, attaining broader
applied utility from ICC requires a viewpoint shift \wrt its capabilities---\ie
fundamentally seeing it as more than tool for complexity-theoretic
reasoning---and the continued exploration of its applied potential.

The ICC approach of guaranteeing program properties demonstrates great applied
potential. For example, if offers a pathway toward formal verification of many
non-functional properties\index{non-functional property}. One research
direction, that deserves more attention, is applying ICC systems to ensure
properties by construction. This would provide guarantees before any program
exists and eliminates the need for \emph{aposteriori} analysis. Although the
dissertation research did not extend that far, exploring this direction is an
intriguing future goal. Whether it is attainable depends crucially on a
viewpoint shift. We should regard \emph{implicit computational complexity} in
the broader context it offers---including as a rich toolbox of techniques for
\ndx{static program analysis}---and continue the exploration of its applied
potential.

% https://stackoverflow.com/questions/3492188/what-are-the-practical-limitations-of-a-non-turing-complete-language-like-coq
% https://stackoverflow.com/questions/315340/practical-non-turing-complete-languages
% ``a general-purpose language that isn't turing complete is pretty impractical.'' or so they say.
% Initial comment on https://stackoverflow.com/q/315340
% Thought: (afterward) comment on  https://stackoverflow.com/q/315340
% Just having Turing-completeness is not super useful (we can show this)
% As language designers we want to add restrinctions... maybe I can find this again to capture the precise comment.

% The ICC approach of guaranteeing program properties demonstrates great applied potential;
% for example, toward formal verification of non-functional properties.
% One investigative direction that deserves more attention is applying ICC to guarantee properties by construction.
% This would provide guarantees before any program exists and eliminates the need for \emph{aposteriori} analysis.
% Although the dissertation did not extend that far, exploring this direction is an intriguing future goal.
% Whether it is attainable depends crucially on a viewpoint shift.
% We should regard ICC in the broader context it offers and continue the exploration of its applied potential.
